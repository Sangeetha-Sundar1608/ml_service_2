"""
Embedding Routes - Text Embedding Endpoints
EPIC 3: Text Embeddings
"""

from fastapi import APIRouter, HTTPException, Depends, BackgroundTasks
from typing import Dict, Any
import logging
import uuid
import time

from models.request_models import EmbeddingRequest, BatchEmbeddingRequest
from models.response_models import EmbeddingResponse, BatchEmbeddingResponse, JobStatusResponse
from services.embedding_service import EmbeddingService
from services.cost_tracker import CostTracker
from main import get_embedding_service, get_cost_tracker, get_db
from sqlalchemy.orm import Session

# Import your existing Celery task
# from tasks import process_batch

logger = logging.getLogger(__name__)

router = APIRouter()


@router.post("/generate", response_model=EmbeddingResponse)
async def generate_embeddings(
    request: EmbeddingRequest,
    embedding_service: EmbeddingService = Depends(get_embedding_service),
    cost_tracker: CostTracker = Depends(get_cost_tracker),
    db: Session = Depends(get_db),
):
    """
    Generate embeddings synchronously
    
    EPIC 3.1: Local Embedding Generation
    EPIC 3.3: OpenAI Embedding Fallback
    
    **Request:**
```json
    {
      "texts": ["Hello world", "Machine learning"],
      "model": "all-MiniLM-L6-v2",
      "normalize": true
    }
```
    
    **Response:**
```json
    {
      "embeddings": [[0.1, 0.2, ...], [0.3, 0.4, ...]],
      "model": "all-MiniLM-L6-v2",
      "provider": "local",
      "tokens": 10,
      "dimension": 384,
      "processing_time_ms": 50
    }
```
    """
    try:
        if not request.texts:
            raise HTTPException(status_code=400, detail="texts cannot be empty")
        
        if len(request.texts) > 1000:
            raise HTTPException(
                status_code=400,
                detail="Max 1000 texts for sync generation. Use /batch for larger requests."
            )
        
        logger.info(f"Generating embeddings for {len(request.texts)} texts")
        
        # Generate embeddings
        result = await embedding_service.generate(
            texts=request.texts,
            normalize=request.normalize,
            use_fallback=True,
        )
        
        # Track usage if OpenAI was used
        if result["provider"] == "openai":
            await cost_tracker.track_embedding_usage(
                db=db,
                provider="openai",
                model=result["model"],
                tokens=result["tokens"],
                latency_ms=result["processing_time_ms"],
                success=True,
            )
        
        logger.info(
            f"âœ… Embeddings generated: {len(request.texts)} texts, "
            f"provider={result['provider']}, {result['processing_time_ms']}ms"
        )
        
        return EmbeddingResponse(**result)
    
    except HTTPException:
        raise
    
    except Exception as e:
        logger.error(f"Embedding generation failed: {e}", exc_info=True)
        raise HTTPException(
            status_code=500,
            detail={
                "error": "Embedding generation failed",
                "message": str(e),
            }
        )


@router.post("/batch", response_model=BatchEmbeddingResponse, status_code=202)
async def batch_embeddings(
    request: BatchEmbeddingRequest,
    background_tasks: BackgroundTasks,
):
    """
    Generate embeddings asynchronously (batch)
    
    EPIC 3.2: Batch Embedding with Optimization
    
    **Request:**
```json
    {
      "texts": ["text1", "text2", ...],  // up to 10,000
      "webhook_url": "https://callback.example.com",
      "callback_headers": {"Authorization": "Bearer token"}
    }
```
    
    **Response (202 Accepted):**
```json
    {
      "job_id": "550e8400-e29b-41d4-a716-446655440000",
      "status": "queued",
      "estimated_time_seconds": 30
    }
```
    """
    try:
        if not request.texts:
            raise HTTPException(status_code=400, detail="texts cannot be empty")
        
        if len(request.texts) > 10000:
            raise HTTPException(
                status_code=400,
                detail="Max 10,000 texts allowed"
            )
        
        # Generate job ID
        job_id = str(uuid.uuid4())
        
        logger.info(f"Batch embedding job created: {job_id} ({len(request.texts)} texts)")
        
        # Queue job (you can integrate with your existing Celery setup)
        # For now, we'll use BackgroundTasks as a simple alternative
        
        # Estimate time (30s per 1000 texts)
        estimated_time = max(5, int((len(request.texts) / 1000) * 30))
        
        # TODO: Integrate with your existing process_batch Celery task
        # process_batch.apply_async(args=[job_id], task_id=job_id)
        
        # For now, return response
        return BatchEmbeddingResponse(
            job_id=job_id,
            status="queued",
            estimated_time_seconds=estimated_time,
        )
    
    except HTTPException:
        raise
    
    except Exception as e:
        logger.error(f"Batch embedding job creation failed: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.get("/batch/{job_id}", response_model=JobStatusResponse)
async def get_batch_status(job_id: str):
    """
    Get batch embedding job status
    
    **Response:**
```json
    {
      "job_id": "550e8400-...",
      "status": "processing",
      "progress_percent": 45,
      "result": null,
      "error": null,
      "created_at": "2025-01-28T10:00:00Z",
      "updated_at": "2025-01-28T10:00:30Z"
    }
```
    """
    # TODO: Integrate with your existing job tracking
    # For now, return mock response
    
    raise HTTPException(
        status_code=501,
        detail="Batch status endpoint - integrate with your existing Celery job tracker"
    )


@router.get("/model/info")
async def get_model_info(
    embedding_service: EmbeddingService = Depends(get_embedding_service),
):
    """
    Get embedding model information
    
    **Response:**
```json
    {
      "name": "all-MiniLM-L6-v2",
      "loaded": true,
      "dimension": 384,
      "first_load_time_s": 2.5
    }
```
    """
    try:
        info = embedding_service.get_model_info()
        return info
    
    except Exception as e:
        logger.error(f"Failed to get model info: {e}")
        raise HTTPException(status_code=500, detail=str(e))